{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_images.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg_cvjlAocLC"
      },
      "source": [
        "**<h1>Glasses or No Glasses distributed (Image Dataset)</h1>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjhSBHc9qYha"
      },
      "source": [
        "In this notebook we will try to face the [Glasses or No Glasses](https://www.kaggle.com/jeffheaton/glasses-or-no-glasses) challenge by Kaggle in a distributed way, mainly using [PySpark](https://spark.apache.org/docs/latest/api/python/index.html), a Python interface for Apache Spark. The challenge provides two datasets, a numerical one and an image one. We will deal with the image dataset, using it to train a CNN.\n",
        "Refer to [this notebook](https://github.com/flaviofuria/GlassesOrNoGlasses/blob/main/glasses_no_glasses.ipynb), in which we faced the challenge (using both the dataset) with the classic data analysis/machine learning tools, to understand all the analysis, the reasoning and the choice of the network architecture. For the distributed version of the MLP applied on the numerical dataset, refer to [this notebook](https://github.com/flaviofuria/GlassesOrNoGlasses/blob/main/pyspark_numerical.ipynb) instead.\n",
        "\n",
        "---\n",
        "First we need some minutes to install [Elephas](https://github.com/maxpumperla/elephas), an extension of [Keras](https://keras.io/) that brings deep learning to Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URbdB5bmxFca",
        "outputId": "f702245b-6f04-4604-d2ad-802f103f1d9e"
      },
      "source": [
        "!pip -q install elephas\n",
        "!pip -q install tensorflow==2.6.0\n",
        "!pip -q install keras==2.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.1 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 53 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.0 MB/s \n",
            "\u001b[?25h  Building wheel for elephas (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 9.5 kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 21.7 MB/s \n",
            "\u001b[?25h  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "elephas 3.0.0 requires h5py==3.3.0, but you have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.2 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "elephas 3.0.0 requires h5py==3.3.0, but you have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdY27i5kn6Rc"
      },
      "source": [
        "**<h4>Running Spark</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-czmhrYSiB7X"
      },
      "source": [
        "The following steps are required to run Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi-ik1XEfPGc"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.0.3-bin-hadoop2.7\")\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfyW5qVPh9Rl"
      },
      "source": [
        "**<h4>Downloading the data</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO8EhZJo7qug"
      },
      "source": [
        "\n",
        "Now we download the data using the Kaggle API. To do so, we replace `KAGGLE_USERNAME` and `KAGGLE_KEY` variables with our Kaggle credentials. Then, we unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnxSEpuGffGU",
        "outputId": "e37f90ea-df52-4823-d694-6066e79e7c82"
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"xxxxxxxxxxxx\"                     \n",
        "os.environ['KAGGLE_KEY'] = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "!kaggle datasets download -d jeffheaton/glasses-or-no-glasses\n",
        "!unzip -o -q /content/glasses-or-no-glasses.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading glasses-or-no-glasses.zip to /content\n",
            "100% 6.11G/6.11G [02:08<00:00, 25.8MB/s]\n",
            "100% 6.11G/6.11G [02:09<00:00, 50.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LM9-vXS8VdI"
      },
      "source": [
        "We delete the stuff we don't need anymore, in order to save some space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VleAQc-0NTZ"
      },
      "source": [
        "!rm glasses-or-no-glasses.zip\n",
        "!rm spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!rm -r sample_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M51KVGIgoBCL"
      },
      "source": [
        "**<h4>Reproducibility</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9rj4lt6iHVt"
      },
      "source": [
        "Next, we set all the `seed` variables, in order to allow reproducibility of our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIGeFZ8sfdlr"
      },
      "source": [
        "seed = 0\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
        "\n",
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3bNgU8MiMCV"
      },
      "source": [
        "**<h4>Downloading our own labels</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT2rQxjG8frs"
      },
      "source": [
        "We can start to get all the data we need, starting from labels. Being that the available labels in the original dataset were wrong and incomplete, we decided to write our own label structure. We download it from the project Github repository and store them as a `Spark DataFrame`, specifying the `schema`, i.e. the structure of the DataFrame we want to create."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7bheN_CfgxC"
      },
      "source": [
        "correct_labels_url = \"https://raw.githubusercontent.com/flaviofuria/GlassesOrNoGlasses/main/labels.csv\"\n",
        "spark.sparkContext.addFile(correct_labels_url)\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, ArrayType\n",
        "schema = StructType([StructField(\"index\", IntegerType(), False), StructField(\"label\", DoubleType(), True)])\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "labels_df = spark.read.csv(SparkFiles.get(\"labels.csv\"), header=True, schema=schema)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "-DAT4fhJZjzW",
        "outputId": "bb9ce1e1-71f9-477c-9777-6913c4a4bc64"
      },
      "source": [
        "labels_df.limit(5).toPandas()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  label\n",
              "0      1    0.0\n",
              "1      2    1.0\n",
              "2      3    1.0\n",
              "3      4    0.0\n",
              "4      5    0.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDiYLo-aiN7g"
      },
      "source": [
        "**<h4>Creating the Dataset</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MytK38lc9LD_"
      },
      "source": [
        "Next, we store the actual dataset in another Spark DataFrame. In this case we let Spark automatically infer the schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYBCehoEgaHz"
      },
      "source": [
        "images_df = spark.read.format(\"binaryFile\").load(\"/content/faces-spring-2020/faces-spring-2020/\", inferSchema=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "GESF5iL59cdd",
        "outputId": "2bc49110-7fe6-4561-9b26-069c687e93ea"
      },
      "source": [
        "images_df.limit(5).toPandas()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>modificationTime</th>\n",
              "      <th>length</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>file:/content/faces-spring-2020/faces-spring-2...</td>\n",
              "      <td>2020-04-18 00:09:48</td>\n",
              "      <td>1738798</td>\n",
              "      <td>[137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>file:/content/faces-spring-2020/faces-spring-2...</td>\n",
              "      <td>2020-04-18 00:07:40</td>\n",
              "      <td>1635390</td>\n",
              "      <td>[137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>file:/content/faces-spring-2020/faces-spring-2...</td>\n",
              "      <td>2020-04-18 00:08:30</td>\n",
              "      <td>1633872</td>\n",
              "      <td>[137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>file:/content/faces-spring-2020/faces-spring-2...</td>\n",
              "      <td>2020-04-18 00:07:22</td>\n",
              "      <td>1633602</td>\n",
              "      <td>[137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>file:/content/faces-spring-2020/faces-spring-2...</td>\n",
              "      <td>2020-04-18 00:10:06</td>\n",
              "      <td>1632559</td>\n",
              "      <td>[137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  ...                                            content\n",
              "0  file:/content/faces-spring-2020/faces-spring-2...  ...  [137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...\n",
              "1  file:/content/faces-spring-2020/faces-spring-2...  ...  [137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...\n",
              "2  file:/content/faces-spring-2020/faces-spring-2...  ...  [137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...\n",
              "3  file:/content/faces-spring-2020/faces-spring-2...  ...  [137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...\n",
              "4  file:/content/faces-spring-2020/faces-spring-2...  ...  [137, 80, 78, 71, 13, 10, 26, 10, 0, 0, 0, 13,...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFjVvCj-oMVV"
      },
      "source": [
        "**<h4>Preprocessing</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laE1C9G99feL"
      },
      "source": [
        "At this point the dataset is just made of binary files, but we need images. To be more precise, we need pixel values to feed the CNN. Thus, we need to preprocess the data. The steps we need to take are:\n",
        "1. We need to link images and labels. The labels indices are associated to the unique number in the path of each image. Thus, we read the path of every image and get this number, storing it in the `id` column of the dataframe. We use `UDFs` (user defined function), that allow us to perform operations not available among Spark SQL functions.\n",
        "2. Now that we have ids, we use them to `join` images and labels. This operation is heavy but required to make the CNN learn in a supervised way and, of course, to subsequently evaluate the predictions.\n",
        "3. We `filter` the dataset, removing images with an undefined label (images are created by GANs and some errors can make impossible to assign a unique label to them).\n",
        "4. We use another UDF that given a `bytearray` returns a `DenseVector` of size `width*height` (our experiments suggested to use `128` for both and to use the grayscale version of the original RGB images). In this step we also normalize each pixel value, dividing it by `255`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PLGpx-hgcsK"
      },
      "source": [
        "import re\n",
        "def get_id_from_path(path:str) -> int:\n",
        "  return int(re.findall(r\"\\d+\", path)[2])\n",
        "\n",
        "\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "import cv2\n",
        "import io\n",
        "def get_dense_from_data(img_data):\n",
        "  width, height, num_channels = 128, 128, 1\n",
        "  return Vectors.dense((cv2.resize(cv2.imdecode(np.array(img_data), cv2.IMREAD_GRAYSCALE), (width, height))/255).flatten())\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "get_id_from_path_udf = udf(lambda x: get_id_from_path(x), IntegerType())\n",
        "get_dense_from_data_udf = udf(lambda x: get_dense_from_data(x), VectorUDT())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOJ7cfLMLeAG"
      },
      "source": [
        "We now apply all the transformations we have just shown, obtaining the preprocessed dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHk2pbGstfHe"
      },
      "source": [
        "indexed_images_df = images_df.withColumn(\"id\", get_id_from_path_udf(\"path\"))\n",
        "labeled_images_df = indexed_images_df.join(labels_df, indexed_images_df.id==labels_df.index, \"inner\")\n",
        "no_undefined_images_df = labeled_images_df.filter((labeled_images_df.label==0.0)|(labeled_images_df.label==1.0)).drop(\"index\")\n",
        "featurized_images_df = no_undefined_images_df.withColumn(\"features\", get_dense_from_data_udf(\"content\"))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLF5QIncXlPe",
        "outputId": "d156f469-a535-4795-a73d-92ea7e260f43"
      },
      "source": [
        "featurized_images_df.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-------+--------------------+----+-----+--------------------+\n",
            "|                path|   modificationTime| length|             content|  id|label|            features|\n",
            "+--------------------+-------------------+-------+--------------------+----+-----+--------------------+\n",
            "|file:/content/fac...|2020-04-18 00:09:48|1738798|[89 50 4E 47 0D 0...|3643|  1.0|[0.20784313725490...|\n",
            "|file:/content/fac...|2020-04-18 00:07:40|1635390|[89 50 4E 47 0D 0...|2815|  1.0|[0.21568627450980...|\n",
            "|file:/content/fac...|2020-04-18 00:08:30|1633872|[89 50 4E 47 0D 0...| 315|  1.0|[0.25490196078431...|\n",
            "|file:/content/fac...|2020-04-18 00:07:22|1633602|[89 50 4E 47 0D 0...|2690|  0.0|[0.49803921568627...|\n",
            "|file:/content/fac...|2020-04-18 00:10:06|1632559|[89 50 4E 47 0D 0...|3760|  1.0|[0.17254901960784...|\n",
            "+--------------------+-------------------+-------+--------------------+----+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAlUYhOaJrM6"
      },
      "source": [
        "**<h4>Train/Test split</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjhw95uFMBrD"
      },
      "source": [
        "We move to the usual `train/test split`: rows are not sorted by id but we still shuffle the dataset and then randomly split it with a ratio of `0.8/0.2`, obtaining a training and a test dataframes.\n",
        "\n",
        "---\n",
        "The task we are going to perform is pretty heavy, thus we suggest to limit the total number of samples if you are running this on Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLRtdeEtfE4"
      },
      "source": [
        "split = .2\n",
        "\n",
        "from pyspark.sql.functions import rand\n",
        "#featurized_images_df = featurized_images_df.limit(1000)         #    tune the limit value and uncomment this if you want to train on a smaller dataset\n",
        "featurized_images_df = featurized_images_df.orderBy(rand(seed))\n",
        "cached_featurized_images_df = featurized_images_df.cache()\n",
        "\n",
        "train_df, test_df = cached_featurized_images_df.randomSplit([1-split, split], seed=seed)\n",
        "\n",
        "cached_train_df = train_df.cache()\n",
        "cached_test_df = test_df.cache()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRKaekZwk0v1"
      },
      "source": [
        "**<h4>Model</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPstiM_eMqIo"
      },
      "source": [
        "These two functions are used to build a `Keras model` and a `Elephas Estimator`, respectively. An estimator is an algorithm that produces a `Transformer` after training on a DataFrame, while the Transformer can transform a DataFrame of features in a DataFrame of predictions. Elephas builds an Estimator from a Keras Model, giving the possibility to perform distributed deep learning. In particular, distribution is obtained by serializing the model, sending it to the workers, let them train their chunk and send gradients to the driver. A `master model` on the driver uses these gradients together with an optimizer in order to update its weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANuN33s4te99"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "\n",
        "def build_model(layers_list, optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"]):\n",
        "  model = Sequential()\n",
        "  for layer in layers_list:\n",
        "    model.add(layer)\n",
        "  model.compile(optimizer, loss, metrics)\n",
        "  return model\n",
        "\n",
        "\n",
        "def build_estimator(model, epochs, batch_size, categorical, num_workers=1, loss=\"binary_crossentropy\",\n",
        "                    nb_classes=None, optimizer=tf.keras.optimizers.serialize(tf.keras.optimizers.Adam()), split=.2,\n",
        "                    featuresCol=\"features\", labelCol=\"label\", metrics=[\"acc\"], frequency=\"batch\", mode=\"asynchronous\"):\n",
        "  return ElephasEstimator(keras_model_config=model.to_json(), num_workers=num_workers, loss=loss, categorical=categorical,\n",
        "                          nb_classes=nb_classes, optimizer_config=optimizer, validation_split=split, epochs=epochs, batch_size=batch_size,\n",
        "                          featuresCol=featuresCol, labelCol=labelCol, metrics=metrics, frequency=frequency, mode=mode)   "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy37u3h4R9iU"
      },
      "source": [
        "This is the CNN we are going to train with our dataset. In our experiments (refer to [this notebook (manca link)](https://)) it has proven to give great performance and still being not so long to train. The first layer (`Reshape`) is required because we can't store a multi-dimensional array in a Spark Dataframe. Thus, we need to take the DenseVector and reshape it in order to feed it into the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqWtmsDrtfAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a9baee-41fb-4220-e957-075370ab6a5b"
      },
      "source": [
        "width, height, num_channels = 128, 128, 1\n",
        "\n",
        "from tensorflow.keras.layers import Reshape, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "layers_list = [Reshape(target_shape=(width, height, num_channels), input_shape=(width*height, 1)),\n",
        "               Conv2D(kernel_size=3, filters=8, activation='relu', input_shape=(width, height, num_channels)),\n",
        "               MaxPooling2D(strides=2),\n",
        "               Conv2D(kernel_size=5, filters=16, activation='relu'),\n",
        "               MaxPooling2D(strides=2),\n",
        "               Flatten(),\n",
        "               Dense(32, activation='relu'),\n",
        "               Dense(16, activation='relu'),\n",
        "               Dense(1, activation=\"sigmoid\")]\n",
        "\n",
        "model = build_model(layers_list)\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 128, 128, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 126, 126, 8)       80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 63, 63, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 59, 59, 16)        3216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 13456)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                430624    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 434,465\n",
            "Trainable params: 434,465\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9X-cSdvl02s"
      },
      "source": [
        "**<h4>Elephas Estimator</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZR_HEzoTD7K"
      },
      "source": [
        "We set some of the `hyperparameters` and then build the estimator. It takes the Keras model, together with these hyperparameters, the name of the features and labels columns of the training dataframe and three other parameters that actually control the distribution: \n",
        "* `frequency`, i.e. how often updates are sent to the master node. It can be done after every `batch` or `epoch`.\n",
        "* `mode`, which can be `synchronous`, if workers send their updates all at the same time, `asynchronous` if they send updates whenever they are ready (locks are used to not lose any update) and `hogwild`, which is similar to the second one but without the locks (losing some updates is ok if this speeds the computation up). Asynchronous is our choice.\n",
        "* the number of actual workers that will participate to the distributed training step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9_oBpekte7d"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "estimator = build_estimator(model=model, epochs=epochs, batch_size=batch_size, num_workers=4, mode=\"asynchronous\", categorical=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSiQUysxl5mf"
      },
      "source": [
        "**</h4>Fit/Transform Step</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxv1JZXHaEeS"
      },
      "source": [
        "We `fit` on the training set and then `transform` on both the training and test set, in order to obtain predictions. We capture the output because there is a lot of logging info that could cause Colab to crash."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpRr93Ahte4D"
      },
      "source": [
        "%%capture\n",
        "fitted_model = estimator.fit(cached_train_df)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W73_F9dte0_"
      },
      "source": [
        "train_predictions = fitted_model.transform(cached_train_df).select(\"prediction\", \"label\")\n",
        "test_predictions = fitted_model.transform(cached_test_df).select(\"prediction\", \"label\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyRIZdpbl_nb"
      },
      "source": [
        "**<h4>Results</h4>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTfiDF6BWGBX"
      },
      "source": [
        "We used a `sigmoid` activation function for the last layer, thus the prediction, which is a value in the unit interval, can be interpreted as the probability to be a positive sample (label 1.0). We give the label `1.0` to samples for which this value is greater than or equal to `0.5`, otherwise we give it `0.0`. The threshold is tunable but if the results are satisfying, we can leave it as is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPmRWT3qU9dN"
      },
      "source": [
        "train_predictions = train_predictions.cache()\n",
        "test_predictions = test_predictions.cache()\n",
        "\n",
        "from pyspark.sql.functions import when, sum, col\n",
        "train_predictions = train_predictions.withColumn(\"predicted_label\", when(train_predictions.prediction[0] >= 0.5, 1.0).otherwise(0.0))\n",
        "test_predictions = test_predictions.withColumn(\"predicted_label\", when(test_predictions.prediction[0] >= 0.5, 1.0).otherwise(0.0))\n",
        "\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "train_metrics = MulticlassMetrics(train_predictions.select(\"predicted_label\", \"label\").rdd.map(tuple))\n",
        "test_metrics = MulticlassMetrics(test_predictions.select(\"predicted_label\", \"label\").rdd.map(tuple))\n",
        "\n",
        "train_accuracy = train_metrics.accuracy\n",
        "test_accuracy = test_metrics.accuracy"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLsjZFwc2lY-",
        "outputId": "b42335a4-cf0b-4c1f-b166-a1ce78202db9"
      },
      "source": [
        "print(f\"Training accuracy: {round(train_accuracy, 4)}\")\n",
        "print(f\"Test accuracy:     {round(test_accuracy, 4)}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 0.9982\n",
            "Test accuracy:     0.9969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7anvynIGmGcq"
      },
      "source": [
        "Pyspark and Elephas do not allow us to get the loss, thus we will compute it on our own. The model has been trained using `binary crossentropy`/`log loss`:\n",
        "$Loss(y,p)=y_{i}*log(p_{i})+(1-y_{i})*log(1-p_{i})).\\;\\;$ We compute this value for all samples and then we average it for all the set\n",
        "\n",
        "---\n",
        "We do the same also for `stable binary crossentropy`, which is the loss used by Tensorflow to avoid computing the logarithm when the predicted probability is too close to 0, which can lead to numerical instability:<br>\n",
        "$Loss(y,z)=max(z,0)-zy+log(1+e^{-|z|})\\;\\;$ with `z` being the output of the network, thus we obtain it by computing the inverse of the sigmoid function (i.e. the `logit`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI4F0R-Fl0ae"
      },
      "source": [
        "from pyspark.sql.functions import log, mean, abs\n",
        "from functools import reduce\n",
        "from math import e\n",
        "\n",
        "def logit(p):\n",
        "  return log(p/(1-p))\n",
        "\n",
        "binary_crossentropy = reduce(lambda p,y: -(y*log(p[0]) + (1-y)*log(1-p[0])), map(col, [\"prediction\", \"label\"]))\n",
        "stable_binary_crossentropy = reduce(lambda p,y: when(logit(p[0])>0, logit(p[0])).otherwise(0) - (logit(p[0])*y) + log(1+e**(-abs(logit(p[0])))), map(col, [\"prediction\", \"label\"]))\n",
        "\n",
        "train_predictions = train_predictions.withColumn(\"binary_crossentropy\", binary_crossentropy)\n",
        "train_predictions = train_predictions.withColumn(\"stable_binary_crossentropy\", stable_binary_crossentropy)\n",
        "test_predictions = test_predictions.withColumn(\"binary_crossentropy\", binary_crossentropy)\n",
        "test_predictions = test_predictions.withColumn(\"stable_binary_crossentropy\", stable_binary_crossentropy)\n",
        "\n",
        "train_binary_crossentropy = train_predictions.select(\"binary_crossentropy\").agg(mean(\"binary_crossentropy\"))\n",
        "train_stable_binary_crossentropy = train_predictions.select(\"stable_binary_crossentropy\").agg(mean(\"stable_binary_crossentropy\"))\n",
        "test_binary_crossentropy = test_predictions.select(\"binary_crossentropy\").agg(mean(\"binary_crossentropy\"))\n",
        "test_stable_binary_crossentropy = test_predictions.select(\"stable_binary_crossentropy\").agg(mean(\"stable_binary_crossentropy\"))\n",
        "\n",
        "\n",
        "train_bin_cross = train_binary_crossentropy.take(1)[0][0]\n",
        "train_stable_bin_cross = train_stable_binary_crossentropy.take(1)[0][0]\n",
        "test_bin_cross = test_binary_crossentropy.take(1)[0][0]\n",
        "test_stable_bin_cross = test_stable_binary_crossentropy.take(1)[0][0]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuCh5H9rl0XD",
        "outputId": "40478a1c-019d-410b-f530-5ecb1145d3b2"
      },
      "source": [
        "print(f\"Training set log loss: {round(train_bin_cross, 4)}\")\n",
        "print(f\"Training set log loss: {round(train_stable_bin_cross, 4)}\")\n",
        "print('')\n",
        "print(f\"Test set log loss:     {round(test_bin_cross, 4)}\")\n",
        "print(f\"Test set log loss:     {round(test_stable_bin_cross, 4)}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set log loss: 0.0099\n",
            "Training set log loss: 0.0099\n",
            "\n",
            "Test set log loss:     0.0118\n",
            "Test set log loss:     0.0118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j0M6CwRkPIy"
      },
      "source": [
        "We don't have instability issues, thus the results are just the same. However, they're different from what we got in the non-distributed implementation. One of the reasons is of course that the the training/test set are not exactly the same, since in both cases we have shuffled the original dataset and then randomly splitted it. Also, the fact that every worker locally computes its gradients and then sends them to the master node, which in turn makes an overall computation with the received ones, could have led to some discrepancy."
      ]
    }
  ]
}
